# ğŸ›¡ï¸ Toxic Comment Detection App

This project is a multi-label **Toxic Comment Classifier** that detects whether a user comment contains toxic, severe toxic, obscene, threat, insult, or identity hate language.  
It uses both traditional ML (TF-IDF + Logistic Regression) and deep learning (Toxic-BERT from Hugging Face).

---

## ğŸ§  Project Highlights

| Feature                     | Details |
|----------------------------|---------|
| ğŸ”¤ Input                   | Raw text comments |
| ğŸ“¦ Models                  | Logistic Regression, BERT (Hugging Face `unitary/toxic-bert`) |
| ğŸ§ª Tasks                   | Multi-label classification (6 classes) |
| ğŸ“Š Evaluation              | Precision, Recall, F1-Score |
| ğŸ“ˆ Visuals                 | Classification reports + ROC & AUC graphs |
| ğŸ›ï¸ Model Toggle           | Compare predictions between Logistic Regression and BERT |
| ğŸŒ Interface               | Built using Streamlit |
| âœ… Real-time inference     | Paste text â†’ get predictions + confidence scores |

---

## ğŸš€ Demo

âš¡ Hosted on Streamlit:  
ğŸ”— [YOUR DEPLOYMENT URL WILL GO HERE]

---

## ğŸ“‚ Folder Structure

toxic-comment-detector/ â”œâ”€â”€ app/ # Streamlit app â”‚ â””â”€â”€ app.py â”œâ”€â”€ models/ # Saved vectorizer and models â”‚ â”œâ”€â”€ tfidf_vectorizer.pkl â”‚ â””â”€â”€ logistic_regression_model.pkl â”œâ”€â”€ outputs/ # Model predictions for comparison â”‚ â”œâ”€â”€ logreg_probs.npy â”‚ â””â”€â”€ toxic_bert_preds.npy â”œâ”€â”€ notebooks/ # All training and comparison notebooks â”‚ â”œâ”€â”€ 1_baseline_model.ipynb â”‚ â”œâ”€â”€ 2_toxic_bert_inference.ipynb â”‚ â””â”€â”€ 3_model_comparison_and_visualization.ipynb â”œâ”€â”€ data/ â”‚ â””â”€â”€ train.csv # Toxic comment dataset â”œâ”€â”€ requirements.txt â””â”€â”€ README.md

yaml
Copy
Edit

---

## ğŸ§ª Evaluation Summary

| Category       | Logistic Regression | Toxic-BERT |
|----------------|---------------------|------------|
| Toxic          | 0.85                | 0.94       |
| Severe Toxic   | 0.31                | 0.50       |
| Obscene        | 0.85                | 0.88       |
| Threat         | 0.15                | 0.58       |
| Insult         | 0.81                | 0.83       |
| Identity Hate  | 0.21                | 0.69       |

---

## âš™ï¸ Setup & Run

1. Clone this repo  
```bash
git clone https://github.com/your-username/toxic-comment-detector.git
cd toxic-comment-detector
Install dependencies

bash
Copy
Edit
pip install -r requirements.txt
Run the Streamlit app

bash
Copy
Edit
streamlit run app/app.py
ğŸ“š Dataset
Used the Jigsaw Toxic Comment Classification dataset from Kaggle.

ğŸ™Œ Author
Wanshika Patro and Nikhil Gokhale
ğŸ“ MS in Data Science, University at Buffalo